# -*- coding: utf-8 -*-
"""scratchpad

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/Engbasemhamada/fc8b0bced8f14536b56d6cc9986abfe5/scratchpad.ipynb
"""

"""
Final Internship Task - Data Preprocessing, Model Building, Hyperparameter Tuning
Team: GDSC (Prepared by Eng. Bassem Hamada)
"""

# =============================
# ğŸ“¦ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
# =============================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª
import warnings
warnings.filterwarnings("ignore")

# =============================
# ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# =============================
data = pd.read_csv("final_internship_data.csv")

print("âœ… Data Loaded Successfully!")
print("Shape:", data.shape)
print("Columns:", data.columns.tolist())

# =============================
# ğŸ” Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# =============================
print("\nğŸ“‹ Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ØµÙÙˆÙ:")
print(data.head())

print("\nâ„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:")
print(data.info())

# =============================
# ğŸ§¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ©
# =============================
# Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© ÙÙŠ ÙƒÙ„ Ø¹Ù…ÙˆØ¯
print("\nğŸ§© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:")
print(data.isnull().sum())

# Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ Ù‚ÙŠÙ…Ø© Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
data = data[data["fare_amount"].notna()]

# Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ù„Ù…ØªÙˆØ³Ø·
num_cols = data.select_dtypes(include=[np.number]).columns.tolist()
for col in num_cols:
    data[col].fillna(data[col].mean(), inplace=True)

# Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø§Ù„Ù†ØµÙŠØ© Ø¨Ø£ÙƒØ«Ø± Ù‚ÙŠÙ…Ø© ØªÙƒØ±Ø§Ø±Ù‹Ø§
cat_cols = data.select_dtypes(include=["object"]).columns.tolist()
for col in cat_cols:
    data[col].fillna(data[col].mode()[0], inplace=True)

print("\nâœ… Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© - Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©:")
print(data.isnull().sum().sum(), "missing values remaining.")

# =============================
# ğŸ”  ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†ØµÙŠØ© Ø¥Ù„Ù‰ Ø±Ù‚Ù…ÙŠØ©
# =============================
le = LabelEncoder()
for col in cat_cols:
    data[col] = le.fit_transform(data[col])

# =============================
# ğŸ¯ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
# =============================
target = "fare_amount"
X = data.drop(columns=[target])
y = data[target]

# =============================
# âœ‚ï¸ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±
# =============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print("\nğŸ“Š Train/Test Shapes:", X_train.shape, X_test.shape)

# =============================
# âš™ï¸ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‚ÙŠÙ… (Standardization)
# =============================
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
print("âœ… ØªÙ… ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")

# =============================
# ğŸ§  Ø¨Ù†Ø§Ø¡ Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø¨Ø¯Ø¦ÙŠ (Baseline Model)
# =============================
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)

# Ø§Ù„ØªÙ†Ø¨Ø¤
y_pred = rf.predict(X_test)

# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("\nğŸ“ˆ Baseline RandomForest Results:")
print(f"RÂ² Score: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")

# =============================
# ğŸ” Hyperparameter Tuning
# =============================
param_grid = {
    "n_estimators": [100, 200, 300],
    "max_depth": [10, 20, None],
    "min_samples_split": [2, 5, 10]
}

print("\nğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª (Hyperparameter Tuning)...")

grid = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=3,
    scoring="r2",
    n_jobs=-1
)
grid.fit(X_train, y_train)

print("\nâœ… Ø£ÙØ¶Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª ØªÙ… Ø¥ÙŠØ¬Ø§Ø¯Ù‡Ø§:")
print(grid.best_params_)

# =============================
# ğŸ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙØ¶Ù„
# =============================
best_model = grid.best_estimator_
y_pred_best = best_model.predict(X_test)

best_r2 = r2_score(y_test, y_pred_best)
best_rmse = np.sqrt(mean_squared_error(y_test, y_pred_best))

print("\nğŸŒŸ Final Model Performance:")
print(f"Best RÂ² Score: {best_r2:.4f}")
print(f"Best RMSE: {best_rmse:.4f}")

# =============================
# ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù
# =============================
results = pd.DataFrame({
    "Model": ["RandomForestRegressor"],
    "R2_Score": [best_r2],
    "RMSE": [best_rmse],
    "Best_Params": [str(grid.best_params_)]
})
results.to_csv("model_results_summary.csv", index=False)
print("\nğŸ“ Saved results to 'model_results_summary.csv'")

# =============================
# ğŸ‰ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
# =============================
print("\nâœ… Task Completed Successfully by Eng. Bassem Hamada (GDSC Team) âœ…")

